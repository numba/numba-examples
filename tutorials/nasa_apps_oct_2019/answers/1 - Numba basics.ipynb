{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Numba\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why does Numba exist?\n",
    "The idea surrounding Numba is to create the ability to extend Python with Python itself. It gets your code performance/capabilities that you just cannot otherwise achieve without C extensions/Cython/Pythran/&lt;insert extension creation package that calls a compiler here&gt;!\n",
    "\n",
    "## Conventions used throughout this tutorial series:\n",
    "\n",
    " * Tasks (exercises for the reader) are marked up in bold blue, like this:<h3><span style=\"color:blue\"> Task 1 </span></h3><br>\n",
    "\n",
    " * Hints/tips/suggestions/things to think about are in bold green, like this:\n",
    "<h4><span style=\"color:green\"> HINT: this is a hint </span></h4>\n",
    "\n",
    "\n",
    "## What is Numba?!\n",
    "\n",
    "**Numba is a type specialising just-in-time (JIT) compiler for Python functions.**\n",
    "\n",
    "It is most often used via a decorator...\n",
    "\n",
    "Simplest example with the simplest decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import njit\n",
    "\n",
    "@njit\n",
    "def add_one(x):\n",
    "    return x + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above the ``njit`` decorator was imported and applied to a simple function (\"Numba is ... for Python functions\"). No compilation has occurred yet! Let's call the function with an integer:\n",
    "\n",
    "<h3><span style=\"color:blue\"> Task 1: call the `add_one` function with an integer input</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do this! Call `add_one` with an integer here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸŽ‰ Congratulations ðŸŽ‰, you just used one of the world's most powerful compliation chains (LLVM!)!\n",
    "\n",
    "Invoking the function makes compilation take place, this is what the just-in-time part is about (\"Numba is a ... just-in-time compiler\"). No compilation occurs until it is needed, and once it has occurred it is cached in process to be reused. Calling the function again with another integer will use the cached code path, it does not need to recompile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one(-129)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's call function with a float."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one(12.34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This of course works, but a new specialisation was needed, Numba recompiled the function for floats.\n",
    "\n",
    "<h4><span style=\"color:green\">Remember: Numba is a compiler, specialisation leads to optimisation</span></h4>\n",
    "\n",
    "\n",
    "Each time a new **TYPE** of input is encountered a new specialisation for the function is being compiled. We can look at these via the `.signatures` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_one.signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is what the type specilisation part means (Numba is a type specialising ... compiler).\n",
    "\n",
    "It in fact has to work out the type of every variable based on the input types and the operations in the function, this is called \"typing\" a function and happens through a process called \"type inference\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does Numba work?\n",
    "\n",
    "![How Numba Works](How_Numba_Works.png)\n",
    "\n",
    "#### <span style=\"color:green\"> TIP: Understanding type inference is really important, most errors you are going to see are because Numba couldn't figure out what type something should be. <br><br>Numba takes a function in a dynamic language and makes a type-static compiled verion of it!</span> \n",
    "\n",
    "## What did Numba do?!\n",
    "\n",
    "We've seen that Numba does multiple stages of transforms to go from bytecode to machine code. Three places that are important to look at are:\n",
    "1. The Numba intermediate representation (IR) level, this shows type information.\n",
    "   Access it via `.inspect_types` on a Numba JIT decorated function.\n",
    "2. The LLVM IR level, this shows the LLVM IR that Numba generated to pass to LLVM.\n",
    "   Access it via `.inspect_llvm` on a Numba JIT decorated function.\n",
    "3. The assembly level, this shows the disassembly of the compiled object.\n",
    "   Access it via `.inspect_asm` on a Numba JIT decorated function.\n",
    "\n",
    "Each of these methods takes a signature `kwarg` (like one obtained from `.signatures` above) as these representations are also type-specialised.\n",
    "\n",
    "<h3><span style=\"color:blue\"> Task 2: Using the `add_one` function from above, for the `float64` signature take a look at the various levels of transforms.</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first select the `float64` signature\n",
    "my_sig = add_one.signatures[1]\n",
    "print(my_sig)\n",
    "\n",
    "# Level 1, print the types using `.inspect_types()`\n",
    "print(add_one.inspect_types(signature=my_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 2, print the LLVM IR using `.inspect_llvm()`\n",
    "print(add_one.inspect_llvm(signature=my_sig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Level 3, print the disassembly using `.inspect_asm()`\n",
    "print(add_one.inspect_asm(signature=my_sig))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking\n",
    "\n",
    "As we saw above, Numba JIT compiles specialisation of a function based on the input types. This means the first invocation is going to involve compilation, and subsequent are going to be from cache. Let's define a more involved function and look at how to benchmark Numba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "@njit\n",
    "def hypot(x, y):\n",
    "    # Implementation from https://en.wikipedia.org/wiki/Hypot\n",
    "    x = abs(x);\n",
    "    y = abs(y);\n",
    "    t = min(x, y);\n",
    "    x = max(x, y);\n",
    "    t = t / x;\n",
    "    return x * math.sqrt(1+t*t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%timeit` magic is appropriate for timing Numba, it runs repeated loops so the cost of compiling the function is amortized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hypot(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Numba `@njit` decorated functions have a `.py_func` attribute which holds reference to the pure Python function, we can use this to check the Python interpreter performance for the above function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hypot.py_func(3.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> **TIP: The most common mistake make when reporting performance results is reporting the execution time as the compilation time along with the execution time.** </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loops?!\n",
    "\n",
    "NumPy is amazing, but a side effect of the \"vectorisation\" paradigm is having to avoid loops. Numba is a compiler, compilers like loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "N = 1000\n",
    "np.random.seed(0)\n",
    "X = np.random.rand((N * N)).reshape((N,  N))\n",
    "\n",
    "@njit\n",
    "def awkward_sine(a):\n",
    "    return np.imag(np.exp(1j * a))\n",
    "\n",
    "# check result\n",
    "np.testing.assert_allclose(awkward_sine(X), np.sin(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of the NumPy implementation running in the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%timeit awkward_sine.py_func(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of Numba JIT version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit awkward_sine(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:blue\"> Task 3a: Reimplement the above `awkward_sine` function with explicit looping using the template below...</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def awkward_sine_loops(a):\n",
    "    m, n = a.shape\n",
    "    result = np.empty_like(a)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i, j] = np.imag(np.exp(1j * a[i, j]))\n",
    "    return result\n",
    "\n",
    "# check result\n",
    "np.testing.assert_allclose(awkward_sine_loops(X), np.sin(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:blue\"> Task 3b: Check the performance of the loop based function above: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit awkward_sine_loops(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing Performance\n",
    "\n",
    "Above we saw Numba JIT out perform NumPy in both a vectorised and a loop induced version. The loop version was quickest by some margin.\n",
    "\n",
    "#### <span style=\"color:green\"> Discussion: What might happen in this similar case? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def awkward_exp(a):\n",
    "    return np.cosh(a) + np.sinh(a)\n",
    "\n",
    "# check result\n",
    "np.testing.assert_allclose(awkward_exp(X), np.exp(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of the NumPy implementation running in the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit awkward_exp.py_func(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the performance of Numba JIT version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit awkward_exp(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we saw that loops made a difference, try that trick again..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def awkward_exp_loops(a):\n",
    "    m, n = a.shape\n",
    "    result = np.empty_like(a)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i, j] = np.cosh(a[i, j]) + np.sinh(a[i, j])\n",
    "    return result\n",
    "            \n",
    "# time it\n",
    "%timeit awkward_exp_loops(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's switch on `fastmath`, this plumbs in Intel's Short Vector Math library (SVML) which swaps out `libm` functions for vectorized ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "@njit(fastmath=True)\n",
    "def awkward_exp_loops_fastmath(a):\n",
    "    m, n = a.shape\n",
    "    result = np.empty_like(a)\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            result[i, j] = np.cosh(a[i, j]) + np.sinh(a[i, j])\n",
    "    return result\n",
    "\n",
    "           \n",
    "%timeit awkward_exp_loops_fastmath(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That made things a bit quicker but NumPy beat Numba all both cases...\n",
    "\n",
    "#### <span style=\"color:green\"> Answer: Anaconda NumPy ``ufunc`` behaviour is hugely optimised and uses MKL VML internally. </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Tips:\n",
    "\n",
    "Do:\n",
    " * Write good tests for the correctness of your code before using the JIT\n",
    " * Write a good performance harness for your code:\n",
    "   * Use real data/representative input\n",
    "   * Keep track of measurements\n",
    "\n",
    "Do not:\n",
    "\n",
    "* Include compilation time in the execution time.\n",
    "* Drag race tiny functions as an assessment of Numba's ability, Numba has dispatch cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit add_one(1)\n",
    "%timeit add_one.py_func(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drag race fundamental math operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def jit_cos(x):\n",
    "    return math.cos(x)\n",
    "\n",
    "%timeit jit_cos(12.34)\n",
    "%timeit jit_cos.py_func(12.34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Expect Numba to outperform BLAS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def dgemv(A, x):\n",
    "    m, n = A.shape\n",
    "    acc = np.zeros((m,))\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            acc[i] += A[i, j] * x[j]\n",
    "    \n",
    "@njit\n",
    "def call_dot(A, x):\n",
    "    return np.dot(A, x) # Numba will defer this to BLAS::XGEMV\n",
    "\n",
    "%timeit dgemv(X, X[0])\n",
    "%timeit call_dot(X, X[0])\n",
    "%timeit call_dot.py_func(X, X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You wouldn't expect Fortran/C to do any better in the above situations, Numba is the same, internally it tries to make sensible decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of measurement:\n",
    "\n",
    "Find the root of `f(x) = cos(x) + 1`. Use the Newton-Raphson algorithm. Use SciPy `optimize.newton` as a performance baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "\n",
    "def f(x):\n",
    "    return np.cos(x) + 1\n",
    "\n",
    "def dfdx(x):\n",
    "    return -np.sin(x)\n",
    "\n",
    "tol = 1e-7 # convergence tolerance\n",
    "max_it = 50 # maximum iterations\n",
    "x0 = 0.5 # starting guess\n",
    "\n",
    "%timeit optimize.newton(f, x0, fprime=dfdx, tol=tol, maxiter=max_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><span style=\"color:blue\"> Task 4:  Write your own Newton-Raphson implementation based on based on pseudo-code from <a href=\"https://en.wikipedia.org/wiki/Newton%27s_method#Pseudocode\">Wikipedia</a>.</span></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def NR_root(f, dfdx, x0, max_it=max_it, eps=tol):\n",
    "    # Write your implementation here...\n",
    "    converged = False\n",
    "    for i in range(max_it):\n",
    "        y = f(x0)\n",
    "        yp = dfdx(x0)\n",
    "        if (abs(yp) < eps):\n",
    "            break\n",
    "        x1 = x0 - y / yp\n",
    "        if abs(x1 - x0) <= eps:\n",
    "            converged = True\n",
    "            break\n",
    "        x0 = x1\n",
    "    if converged:\n",
    "        return x1\n",
    "    else:\n",
    "        raise RuntimeError(\"Solution did not converge\")\n",
    "\n",
    "# if the above is \"correct\", this will pass\n",
    "np.testing.assert_allclose(NR_root.py_func(f, dfdx, x0), np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit NR_root.py_func(f, dfdx, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the Python function with JIT compiled objective functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_jit = njit(f)\n",
    "dfdx_jit = njit(dfdx)\n",
    "\n",
    "%timeit NR_root.py_func(f_jit, dfdx_jit, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time the JIT compiled function with JIT compiled objective functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit NR_root(f_jit, dfdx_jit, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time SciPy with JIT compiled objective functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit optimize.newton(f_jit, x0, fprime=dfdx_jit, tol=tol, maxiter=max_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's really lean on the fact we have a specialising compiler... performance comes from specialisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def specialize(f, dfdx):\n",
    "    # close over the objective functions so they are \"constant\"\n",
    "    # Numba will spot this and LLVM will inline them into the machine code\n",
    "    @njit\n",
    "    def NR_root(x0, max_it=max_it, eps=tol):\n",
    "        converged = False\n",
    "        for i in range(max_it):\n",
    "            y = f(x0)\n",
    "            yp = dfdx(x0)\n",
    "            if (abs(yp) < eps):\n",
    "                break\n",
    "            x1 = x0 - y/yp\n",
    "            if abs(x1 - x0) <= eps:\n",
    "                converged = True\n",
    "                break\n",
    "            x0 = x1\n",
    "        if converged:\n",
    "            return x1\n",
    "        else:\n",
    "            raise RuntimeError(\"Solution did not converge\")\n",
    "    return NR_root\n",
    "\n",
    "f_NR_root = specialize(f_jit, dfdx_jit)\n",
    "\n",
    "%timeit f_NR_root(x0, max_it, tol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> Discuss: What happened in the above?!</span>\n",
    "#### <span style=\"color:green\"> TIP: Experiment and time things!</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
